{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating NLP Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ual-\n",
      "[nltk_data]     laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def extract_article_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        article_text = soup.get_text()\n",
    "        return article_text\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, nltk_stopwords):\n",
    "        self.nltk_stopwords = nltk_stopwords\n",
    "\n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
    "        return filtered_words\n",
    "\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "\n",
    "def perform_sentiment_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    sentiment_score = doc.sentiment\n",
    "    return sentiment_score\n",
    "\n",
    "\n",
    "def perform_pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "def run_etl(base_url, num_pages):\n",
    "    nltk_stopwords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    word_freq_list = []\n",
    "    entities_list = []\n",
    "    sentiment_scores_list = []\n",
    "    pos_tags_list = []\n",
    "\n",
    "    for page_number in range(1, num_pages + 1):\n",
    "        page_url = f\"{base_url}/page/{page_number}\"\n",
    "\n",
    "        scraper = WebScraper(page_url)\n",
    "        article_text = scraper.extract_article_text()\n",
    "\n",
    "        processor = TextProcessor(nltk_stopwords)\n",
    "        filtered_words = processor.tokenize_and_clean(article_text)\n",
    "\n",
    "        # Top Keywords\n",
    "        top_keywords = Counter(filtered_words)\n",
    "        df_word_freq = pd.DataFrame(top_keywords.items(), columns=[\"Words\", \"Frequencies\"])\n",
    "\n",
    "        # Convert 'Frequencies' column to string type\n",
    "        df_word_freq['Frequencies'] = df_word_freq['Frequencies'].astype(str)\n",
    "\n",
    "        # Named Entity Recognition Entities\n",
    "        entities = extract_named_entities(article_text)\n",
    "        df_entities = pd.DataFrame(entities, columns=[\"Entity\", \"Label\"])\n",
    "\n",
    "        # Sentiment Analysis\n",
    "        sentiment_score = perform_sentiment_analysis(article_text)\n",
    "        df_sentiment_scores = pd.DataFrame([sentiment_score], columns=[\"Sentiment_Score\"])\n",
    "\n",
    "        # Part-of-Speech Tagging Distribution\n",
    "        pos_tags = perform_pos_tagging(article_text)\n",
    "        df_pos_tags = pd.DataFrame(pos_tags, columns=[\"Token\", \"POS\"])\n",
    "\n",
    "        # Combine all results into a single dataframe\n",
    "        combined_df = pd.concat([df_word_freq, df_entities, df_sentiment_scores, df_pos_tags], axis=1)\n",
    "\n",
    "        # Save the combined results to a single CSV file\n",
    "        combined_df.to_csv(f\"page_{page_number}_statistics.csv\", index=False)\n",
    "\n",
    "        # Append dataframes to lists\n",
    "        word_freq_list.append(df_word_freq)\n",
    "        entities_list.append(df_entities)\n",
    "        sentiment_scores_list.append(df_sentiment_scores)\n",
    "        pos_tags_list.append(df_pos_tags)\n",
    "\n",
    "    return word_freq_list, entities_list, sentiment_scores_list, pos_tags_list\n",
    "\n",
    "def calculate_average_and_save_to_csv(dataframes_list, output_csv_path):\n",
    "    # Concatenate the input DataFrames along rows\n",
    "    combined_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "\n",
    "    # Convert all columns to numeric type\n",
    "    combined_df = combined_df.apply(pd.to_numeric, errors='coerce', downcast='float')\n",
    "\n",
    "    # Calculate the average for each column\n",
    "    avg_df = combined_df.mean()\n",
    "\n",
    "    # If there is only one row in the average DataFrame, convert it to a DataFrame with a single column\n",
    "    if avg_df.ndim == 0:\n",
    "        avg_df = pd.DataFrame({'Value': avg_df})\n",
    "\n",
    "    # Save the average results to a CSV file with column names\n",
    "    avg_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://medium.com/tag/technology\"\n",
    "    num_pages = 100  # Set the desired number of pages to scrape\n",
    "\n",
    "    word_freq_list, entities_list, sentiment_scores_list, pos_tags_list = run_etl(base_url, num_pages)\n",
    "\n",
    "    # Calculate and save the average results to a single CSV file\n",
    "    calculate_average_and_save_to_csv(\n",
    "        word_freq_list + entities_list + sentiment_scores_list + pos_tags_list,\n",
    "        \"C:\\\\Users\\\\ual-laptop\\\\OneDrive - University of Arizona\\\\Documents\\\\DA Projects\\\\NLP Final\\\\aggregated_results.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
