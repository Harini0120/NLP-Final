Code Structure
1. WebScraper Class
The WebScraper class is responsible for extracting article text from a given URL. It uses the requests library to fetch HTML content and BeautifulSoup for HTML parsing.

2. TextProcessor Class
The TextProcessor class handles text tokenization and cleaning. It uses the nltk library for tokenization and removes stopwords to focus on meaningful words.

3. NLP Analysis Functions
extract_named_entities: Uses the spacy library to perform Named Entity Recognition (NER) on the article text.

perform_sentiment_analysis: Utilizes the spacy library to calculate the sentiment score of the article.

perform_pos_tagging: Applies part-of-speech tagging to the article text using spacy.

4. ETL Process (run_etl Function)
The run_etl function executes the ETL pipeline. It iterates through a specified number of pages, extracts article text, and performs tokenization, keyword frequency analysis, NER, sentiment analysis, and part-of-speech tagging. Results are stored in separate DataFrames and CSV files.

5. Average Calculation and Saving to CSV (calculate_average_and_save_to_csv Function)
This function takes a list of DataFrames generated by the ETL process, concatenates them, calculates the average for each column, and saves the results to a CSV file. If there is only one row in the average DataFrame, it is converted to a DataFrame with a single column.

Usage
Update the base_url variable to the desired Medium tag URL.
Set the num_pages variable to determine the number of pages to scrape.
Run the script to perform web scraping, NLP analysis, and save the aggregated results.
